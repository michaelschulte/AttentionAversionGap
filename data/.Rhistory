install.packages('psych')
library(psych)
# read data into df
df <- read.csv('http://goo.gl/A35IBl')
# task 5
describeBy(df$DV, df$IV)
install.packages("psych")
# BetC23
# Aufgabe 5
# install psych package and load
#install.packages('psych')
library(psych)
# read data into df
df <- read.csv('http://goo.gl/A35IBl')
# task 5
describeBy(df$DV, df$IV)
x <- c('a', 'b', 'c')
y <- c('A', 'B', 'C')
paste(x,y)
paste(c(x,y))
paste(c(x,'\#',y))
paste(c(x,'\\#',y))
paste(c(x,'\/\#',y))
paste(c(x,'#',y))
paste(c(x,'/\#',y))
paste(c(x,'\\#',y))
paste(c(x,''\'#',y))
paste(c(x,"'\'#",y))
paste0(c(x, \# ,y))
paste0(c(x, '\#' ,y))
paste0(c(x, '\\#' ,y))
paste0(c(x, '\\/#' ,y))
paste0(c(x, '/\#' ,y))
paste0(c(x, "/\#" ,y))
paste0(c(x, "/#" ,y))
paste0(c(x, "\/#" ,y))
(5/6)^3*(1/6)^4.
1-(5/6)^3*(1/6)^4
(5/6)^3*(1/6)*4
1- (5/6)^3*(1/6)*4
(1/6)^4
(5/6)^4
3*(5/6)^2*1/6
1/6 + 5/6*1/6 + 5/6*5/6*1/6
1-(5/6*5/6*5/6)
1-(5/6*5/6*5/6*5/6)
data <- read.table(pipe("pbpaste"), sep="\t", header
)
data <- read.table(pipe("pbpaste"), sep="\t", header=TRUE)
data <- read.table(pipe("pbpaste"), sep="\t", header=TRUE)
head(data)
ggplot(data, aes(x = low, y = high)) +
geom_point()
library(ggplot2)
ggplot(data, aes(x = low, y = high)) +
geom_point()
ggplot(data, aes(x = low, y = high)) +
geom_density()
head(data)
ggplot(data, aes(x = low)) +
geom_density()
ggplot(data, aes(x = low)) +
geom_density() +
geom_density(aes(x = high))
ggplot(data, aes(x = low)) +
geom_density() +
geom_density(aes(x = high, color = red))
ggplot(data, aes(x = low)) +
geom_density() +
geom_density(aes(x = high, color = 'red'))
groups <- read.csv(file = 'https://docs.google.com/spreadsheets/d/1BPMViHODdwxQ_00sqEScHuXDwUW0AHyNVYmr6IHMvgs/pub?output=csv', header = TRUE, stringsAsFactors = FALSE)
groups <- groups[2:dim(groups)[1], 1]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
getwd()
data <- read.table(pipe("pbpaste"), sep="\t", header=TRUE)
data
data <- read.table(pipe("pbpaste"), sep=",", header=TRUE)
data <- read.table(pipe("pbpaste"), sep=",", header=TRUE)
data <- read.table(pipe("pbpaste"), sep=",", header=FALSE)
data <- read.table(pipe("pbpaste"), sep=",", header=FALSE)
data <- read.table(pipe("pbpaste"), sep=",", header=FALSE)
data <- read.table(pipe("pbpaste"), sep="\,", header=FALSE)
\t
US, The Netherlands,
US, US, US,
US,
Taiwan, US,
Germany, US,
Germany, Germany, Switzerland, Denmark,
US, US,
Germany, Germany, Switzerland, The Netherland, Switzerland,
Switzerland, The Netherlands, Germany, Germany Switzerland,
Switzerland, Germany,
United Kingdom, United Kingdom,
US, US,
Switzerland, Austria, Switzerland,
US, US, United Kingdom, US, Spain,
United Kingdom, Sweden,
US, US,
US, United Kingdom, Italy,
US, US,
The Netherlands, US,
US, Germany, Switzerland, US,
Switzerland, Switzerland,
Germany, Germany,
Switzerland, Switzerland, Switzerland, Switzerland,
US,
US, Switzerland, US, US, US,
Switzerland, Austria, Oxford
data <- read.table(pipe("pbpaste"), sep=",", header=FALSE)
View(data)
data <- read.table(pipe("pbpaste"), sep=",", header=FALSE)
data <- read.table(pipe("pbpaste"), sep=",", header=FALSE)
data <- read.table(pipe("pbpaste"), sep=",", header=FALSE)
data <- c(US, The Netherlands, US, US, US,US,Taiwan, US,Germany, US,Germany,Germany, Switzerland, Denmark,US, US,Germany, Germany, Switzerland, The Netherland, Switzerland, Switzerland, The Netherlands, Germany, Germany, Switzerland,Switzerland, Germany,United Kingdom, United Kingdom,US, US,Switzerland, Austria, Switzerland,US, US, United Kingdom,US, Spain,United Kingdom, Sweden,US, US,US, United Kingdom, Italy,US, US,The Netherlands, US,US, Germany, Switzerland, US,Switzerland, Switzerland,Germany, Germany,Switzerland, Switzerland, Switzerland, Switzerland,US,US, Switzerland, US, US, US,Switzerland, Austria, Oxford)
data <- c('US','The Netherlands','US','US','US','US','Taiwan','US','Germany','US','Germany','Germany','Switzerland','Denmark','US','US','Germany','Germany','Switzerland',
'The Netherland','Switzerland','Switzerland','The Netherlands','Germany','Germany','Switzerland','Switzerland','Germany','United Kingdom','United Kingdom','US','US',
'Switzerland','Austria','Switzerland','US','US','United Kingdom','US','Spain','UnitedKingdom','Sweden','US','US','US','United Kingdom','Italy','US','US',
'The Netherlands','US','US','Germany','Switzerland','US','Switzerland','Switzerland','Germany','Germany','Switzerland','Switzerland','Switzerland',
'Switzerland','US','US','Switzerland','US','US','US','Switzerland','Austria','Oxford')
table(data)
data <- c('US','The Netherlands','US','US','US','US','Taiwan','US','Germany','US','Germany','Germany','Switzerland','Denmark','US','US','Germany','Germany','Switzerland',
'The Netherland','Germany','United Kingdom','United Kingdom','US','US',
'Switzerland','Austria','US','US','United Kingdom','US','Spain','United Kingdom','Sweden','US','US','US','United Kingdom','Italy','US','US',
'The Netherlands','US','US','Germany','Switzerland','US','Switzerland','Switzerland','Germany','Germany','Switzerland',
'Switzerland','US','US','Switzerland')
table(data)
table(data)
as.data.frame(table(data))
dat <- as.data.frame(table(data))
dat$flag <- sprintf('![](http://flagpedia.net/data/flags/mini/%s.png)', dat$abbr)
as.data.frame(table(data))
dat$abbr <- c('at','dk','de','it','es','se','ch','tw','nl')
dat$flag <- sprintf('![](http://flagpedia.net/data/flags/mini/%s.png)', dat$abbr)
data <- c('US','The Netherlands','US','US','US','US','Taiwan','US','Germany','US','Germany','Germany','Switzerland','Denmark','US','US','Germany','Germany','Switzerland',
'The Netherland','Germany','United Kingdom','United Kingdom','US','US',
'Switzerland','Austria','US','US','United Kingdom','US','Spain','United Kingdom','Sweden','US','US','US','United Kingdom','Italy','US','US',
'The Netherlands','US','US','Germany','Switzerland','US','Switzerland','Switzerland','Germany','Germany','Switzerland',
'Switzerland','US','US','Switzerland')
dat <- as.data.frame(table(data))
dat$abbr <- c('at','dk','de','it','es','se','ch','tw','nl')
data <- c('US','The Netherlands','US','US','US','US','Taiwan','US','Germany','US','Germany','Germany','Switzerland','Denmark','US','US','Germany','Germany','Switzerland',
'The Netherland','Germany','United Kingdom','United Kingdom','US','US',
'Switzerland','Austria','US','US','United Kingdom','US','Spain','United Kingdom','Sweden','US','US','US','United Kingdom','Italy','US','US',
'The Netherlands','US','US','Germany','Switzerland','US','Switzerland','Switzerland','Germany','Germany','Switzerland',
'Switzerland','US','US','Switzerland')
dat <- as.data.frame(table(data))
data <- c('US','The Netherlands','US','US','US','US','Taiwan','US','Germany','US','Germany','Germany','Switzerland','Denmark','US','US','Germany','Germany','Switzerland',
'The Netherland','Germany','United Kingdom','United Kingdom','US','US',
'Switzerland','Austria','US','US','United Kingdom','US','Spain','United Kingdom','Sweden','US','US','US','United Kingdom','Italy','US','US',
'The Netherlands','US','US','Germany','Switzerland','US','Switzerland','Switzerland','Germany','Germany','Switzerland',
'Switzerland','US','US','Switzerland')
as.data.frame(table(data))
dat$abbr <- c('at','dk','de','it','es','se','ch','tw','nl')
data <- c('US','The Netherlands','US','US','US','US','Taiwan','US','Germany','US','Germany','Germany','Switzerland','Denmark','US','US','Germany','Germany','Switzerland',
'The Netherlands','Germany','United Kingdom','United Kingdom','US','US',
'Switzerland','Austria','US','US','United Kingdom','US','Spain','United Kingdom','Sweden','US','US','US','United Kingdom','Italy','US','US',
'The Netherlands','US','US','Germany','Switzerland','US','Switzerland','Switzerland','Germany','Germany','Switzerland',
'Switzerland','US','US','Switzerland')
dat <- as.data.frame(table(data))
dat$abbr <- c('at','dk','de','it','es','se','ch','tw','nl')
as.data.frame(table(data))
dat$abbr <- c('at','dk','de','it','es','se','ch','tw','nl','uk','us')
dat$flag <- sprintf('![](http://flagpedia.net/data/flags/mini/%s.png)', dat$abbr)
library(knitr)
kable(dat)
sum(dat$Freq)
ggplot(diamonds, aes(x=carat, y=price, color=cut)) + geom_point() + geom_smooth() + coord_cartesian(ylim=c(0, 10000)) + labs(title="Coord_cartesian zoomed in!")
library(ggplot2)
ggplot(diamonds, aes(x=carat, y=price, color=cut)) + geom_point() + geom_smooth() + coord_cartesian(ylim=c(0, 10000)) + labs(title="Coord_cartesian zoomed in!")
library(dplyr)
glimps(diamonds)
library(tidyverse)
glimps(diamonds)
glimpse(diamonds)
glimpse(diamonds)
ggplot(diamonds, aes(x=carat, y=price, color=cut)) +
geom_point() +
geom_smooth() +
coord_cartesian(ylim=c(0, 10000)) +
labs(title="Coord_cartesian zoomed in!")
groups <- read.csv(file = 'https://docs.google.com/spreadsheets/d/1BPMViHODdwxQ_00sqEScHuXDwUW0AHyNVYmr6IHMvgs/pub?output=csv', header = TRUE, stringsAsFactors = FALSE)
groups
groups <- groups[2:dim(groups)[1], 1]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
groups[sample(1:length(groups),1)]
library(ggthemes)
install.pacakges('ggthemes')
install.packages('ggthemes')
library*ggthemes
library(ggthemes)
library(ggplot2)
ggplot(mpt, aes(x = manufacturer, y = mpg)) +
geom_point()
ggplot(mpg, aes(x = manufacturer, y = mpg)) +
geom_point()
library(ggplot2)
ggplot(mpg, aes(x = manufacturer, y = mpg)) +
geom_point()
head(mpg)
ggplot(subset(mpg, manufacturere == 'ford', aes(x = manufacturer, y = mpg)) +
geom_point()
)
ggplot(subset(mpg, manufacturere == 'ford'), aes(x = manufacturer, y = mpg)) +
geom_point()
ggplot(subset(mpg, manufacturere == 'ford'), aes(x = manufacturer, y = mpg)) +
geom_point()
ggplot(subset(mpg, manufacturer == 'ford'), aes(x = manufacturer, y = mpg)) +
geom_point()
ggplot(subset(mpg, manufacturer == 'ford'), aes(x = cty, y = mpg)) +
geom_point()
subset(mpg, manufacturer == 'ford')
ggplot(subset(mpg, manufacturer == 'ford'), aes(x = cty, y = mpg)) +
geom_point()
subset(mpg, manufacturer == 'ford')
ggplot(subset(mpg, manufacturer == 'ford'), aes(x = cty, y = hwy)) +
geom_point()
ggplot(subset(mpg, manufacturer == 'ford'), aes(x = cty, y = hwy)) +
geom_point() +
ggtitle('Ford')
library(ggplot2)
ggplot(mpg, aes(x = cty, y = hwy)) +
geom_point() +
ggtitle('Ford')
ggplot(mpg, aes(x = cty, y = hwy, color = manufacturer)) +
geom_point() +
ggtitle('Ford')
ggplot(subset(mpg, manufacturer == 'ford'), aes(x = cty, y = hwy, color = manufacturer)) +
geom_point() +
ggtitle('Ford')
head(F)
head(subset(mpg, manufacturer == 'ford'))
subset(mpg, manufacturer == 'ford')
?mpg
ggplot(subset(mpg, manufacturer == 'ford'), aes(x = cty, y = hwy, color = manufacturer)) +
geom_point() +
ggplot(subset(mpg, manufacturer == 'ford'), aes(x = cty, y = hwy, color = manufacturer)) +
geom_point() +
ggtitle('Ford')
ggplot(subset(mpg, manufacturer == 'ford'), aes(x = cty, y = hwy, color = manufacturer)) +
geom_point() +
ggtitle('Ford')
# Random pick of group
# Michael Schulte-Mecklenbeck
# generate string with group names
groups <- read.csv(file = 'https://docs.google.com/spreadsheets/d/1BPMViHODdwxQ_00sqEScHuXDwUW0AHyNVYmr6IHMvgs/pub?output=csv', header = TRUE, stringsAsFactors = FALSE)
# extract group names
groups <- groups[2:dim(groups)[1], 1]
# generate random number
groups[sample(1:length(groups),1)]
# Random pick of group
# Michael Schulte-Mecklenbeck
# generate string with group names
groups <- read.csv(file = 'https://docs.google.com/spreadsheets/d/1BPMViHODdwxQ_00sqEScHuXDwUW0AHyNVYmr6IHMvgs/pub?output=csv', header = TRUE, stringsAsFactors = FALSE)
# extract group names
groups <- groups[2:dim(groups)[1], 1]
# generate random number
groups[sample(1:length(groups),1)]
# Random pick of group
# Michael Schulte-Mecklenbeck
# generate string with group names
groups <- read.csv(file = 'https://docs.google.com/spreadsheets/d/1BPMViHODdwxQ_00sqEScHuXDwUW0AHyNVYmr6IHMvgs/pub?output=csv', header = TRUE, stringsAsFactors = FALSE)
# extract group names
groups <- groups[2:dim(groups)[1], 1]
# generate random number
groups[sample(1:length(groups),1)]
library(RTextTools)
library(e1071)
installed.packages('RTextTools')
installed.packages('e1071')
library(RTextTools)
installed.packages('RTextTools')
install.packages('RTextTools')
install.packages('e1071')
library(RTextTools)
library(e1071)
pos_tweets =  rbind(
c('I love this car', 'positive'),
c('This view is amazing', 'positive'),
c('I feel great this morning', 'positive'),
c('I am so excited about the concert', 'positive'),
c('He is my best friend', 'positive')
)
pos_tweets
neg_tweets = rbind(
c('I do not like this car', 'negative'),
c('This view is horrible', 'negative'),
c('I feel tired this morning', 'negative'),
c('I am not looking forward to the concert', 'negative'),
c('He is my enemy', 'negative')
)
test_tweets = rbind(
c('feel happy this morning', 'positive'),
c('larry friend', 'positive'),
c('not like that man', 'negative'),
c('house not great', 'negative'),
c('your song annoying', 'negative')
)
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
tweets
matrix= create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
matrix
matrix <- create_matrix(tweets[,1], language="english",
removeStopwords=FALSE, removeNumbers=TRUE,
stemWords=FALSE)
matrix
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:10,], as.factor(tweets[1:10,2]) )
mat <- as.matrix(matrix)
classifier <- naiveBayes(mat[1:10,], as.factor(tweets[1:10,2]) )
predicted <- predict(classifier, mat[11:15,]); predicted
predicted
table(tweets[11:15, 2], predicted)
recall_accuracy(tweets[11:15, 2], predicted)
container <- create_container(matrix, as.numeric(as.factor(tweets[,2])),
trainSize=1:10, testSize=11:15,virgin=FALSE)
models <- train_models(container, algorithms=c("MAXENT" , "SVM", "RF", "BAGGING", "TREE"))
results <- classify_models(container, models)
table(as.numeric(as.factor(tweets[11:15, 2])), results[,"FORESTS_LABEL"])
table(as.numeric(as.factor(tweets[11:15, 2])), results[,"MAXENTROPY_LABEL"])
# recall accuracy
recall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,"FORESTS_LABEL"])
recall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,"MAXENTROPY_LABEL"])
recall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,"TREE_LABEL"])
recall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,"BAGGING_LABEL"])
recall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,"SVM_LABEL"])
analytics <- create_analytics(container, results)
summary(analytics)
head(analytics@document_summary)
analytics@ensemble_summar
cross_validate(container,N,"MAXENT")
N <- 4
set.seed(2014)
cross_validate(container,N,"MAXENT")
cross_validate(container,N,"TREE")
cross_validate(container,N,"SVM")
cross_validate(container,N,"RF")
library(readr)
library(dplyr)
library(stringr)
library(jsonlite)
infile <- "~misc/Dropbox (2.0)/1_Teaching/3_Seminare/R_Seminar/NewStuff/yelp_dataset_challenge_round9/"
review_lines <- read_lines(infile, n_max = 200000, progress = TRUE)
View(pos_tweets)
infile <- "~misc/Dropbox (2.0)/1_Teaching/3_Seminare/R_Seminar/NewStuff/yelp_dataset_challenge_round9/"
dir()
infile <- "~misc/Dropbox (2.0)/1_Teaching/3_Seminare/R_Seminar/NewStuff/yelp_dataset_challenge_round9/yelp_academic_dataset_review.json"
review_lines <- read_lines(infile, n_max = 200000, progress = TRUE)
head(review_lines)
review_lines <- read_lines(infile, n_max = 500000, progress = TRUE)
getwd()
setwd("~misc/Dropbox (2.0)/1_Teaching/3_Seminare/R_Seminar/NewStuff/yelp_dataset_challenge_round9"
review_lines <- read_lines(infile, n_max = 500000, progress = TRUE)
writeRDS(review_lines, file = '')
library(stringr)
library(jsonlite)
reviews_combined <- str_c("[", str_c(review_lines, collapse = ", "), "]")
reviews <- fromJSON(reviews_combined) %>%
flatten() %>%
tbl_df()
setwd("~misc/Dropbox (2.0)/1_Teaching/3_Seminare/R_Seminar/NewStuff/yelp_dataset_challenge_round9")
getwd()
review_lines <- read_lines(yelp_academic_dataset_review.json, n_max = 500000, progress = TRUE)
getwd()
dir()
review_lines <- read_lines('yelp_academic_dataset_review.json', n_max = 500000, progress = TRUE)
writeRDS(review_lines, file = 'YelpRaw500k.RDS')
saveRDS(review_lines, file = 'YelpRaw500k.RDS')
setwd("~misc/Dropbox (2.0)/1_Teaching/3_Seminare/R_Seminar/NewStuff/yelp_dataset_challenge_round9")
review_lines <- read_lines('yelp_academic_dataset_review.json', n_max = 200000, progress = TRUE)
saveRDS(review_lines, file = 'YelpRaw200k.RDS')
reviews_combined <- str_c("[", str_c(review_lines, collapse = ", "), "]")
reviews <- fromJSON(reviews_combined) %>%
flatten() %>%
tbl_df()
View(reviews)
library(tidytext)
review_words <- reviews %>%
select(review_id, business_id, stars, text) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words$word,
str_detect(word, "^[a-z']+$"))
install.packages('tidytext')
library(tidytext)
review_words <- reviews %>%
select(review_id, business_id, stars, text) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words$word,
str_detect(word, "^[a-z']+$"))
AFINN <- sentiments %>%
filter(lexicon == "AFINN") %>%
select(word, afinn_score = score)
AFINN
reviews_sentiment <- review_words %>%
inner_join(AFINN, by = "word") %>%
group_by(review_id, stars) %>%
summarize(sentiment = mean(afinn_score))
reviews_sentiment
library(ggplot2)
ggplot(reviews_sentiment, aes(stars, sentiment, group = stars)) +
geom_boxplot() +
ylab("Average sentiment score") +
theme_bw()
cor(reviews_sentiment$stars, reviews_sentiment$sentiment)
review_words_counted <- review_words %>%
count(review_id, business_id, stars, word) %>%
ungroup()
review_words_counted
review_words_counted <- review_words %>%
count(review_id, business_id, stars, word)
review_words_counted
?ungroup
grouped <- group_by(mtcars, cyl)
groups(grouped)
groups(ungroup(grouped))
word_summaries <- review_words_counted %>%
group_by(word) %>%
summarize(businesses = n_distinct(business_id),
reviews = n(),
uses = sum(n),
average_stars = mean(stars)) %>%
ungroup()
word_summaries
word_summaries_filtered <- word_summaries %>%
filter(reviews >= 200, businesses >= 10)
word_summaries_filtered
# What about negative words?
word_summaries_filtered %>%
arrange(average_stars)
words_afinn <- word_summaries_filtered %>%
inner_join(AFINN)
words_afinn
AFINN
ggplot(words_afinn, aes(afinn_score, average_stars, size = reviews)) +
geom_smooth(method="lm", se=FALSE, show.legend=FALSE) +
geom_text(aes(label = word, size = NULL), check_overlap = TRUE, vjust=1, hjust=1) +
geom_point() +
scale_x_continuous(limits = c(-6,6)) +
xlab("AFINN sentiment score") +
ylab("Average Yelp stars")
ggplot(words_afinn, aes(reviews, average_stars, color = afinn_score)) +
geom_point() +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 1) +
scale_x_log10() +
geom_hline(yintercept = mean(reviews$stars), color = "red", lty = 2) +
scale_colour_gradient2("AFINN", low = "red", mid = "white", high = "blue", limits = c(-5,5)) +
xlab("# of reviews") +
ylab("Average Stars")
ggplot(words_afinn, aes(reviews, average_stars, color = afinn_score)) +
geom_point() +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 1) +
scale_x_log10() +
geom_hline(yintercept = mean(reviews$stars), color = "red", lty = 2) +
scale_colour_gradient2("AFINN", low = "red", mid = "white", high = "blue", limits = c(-5,5)) +
xlab("# of reviews") +
ylab("Average Stars") +
theme_bw()
ggplot(words_afinn, aes(reviews, average_stars, color = afinn_score)) +
geom_point() +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 1) +
# scale_x_log10() +
geom_hline(yintercept = mean(reviews$stars), color = "red", lty = 2) +
scale_colour_gradient2("AFINN", low = "red", mid = "white", high = "blue", limits = c(-5,5)) +
xlab("# of reviews") +
ylab("Average Stars") +
theme_bw()
ggplot(words_afinn, aes(reviews, average_stars, color = afinn_score)) +
geom_point() +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 1) +
scale_x_log10() +
geom_hline(yintercept = mean(reviews$stars), color = "red", lty = 2) +
scale_colour_gradient2("AFINN", low = "red", mid = "white", high = "blue", limits = c(-5,5)) +
xlab("# of reviews") +
ylab("Average Stars") +
theme_bw()
library(tidyverse)
setwd('~misc/Dropbox (2.0)/4_ProcessTracingProjects/Loss_Attention/data/')
rm(list=lm())
rm(list=ls())
rm(ls())
list=
load('click_rawclean.Rdata')
clickdata <- load('click_rawclean.Rdata')
clickdata <- rawclean
load('click_rawclean100.Rdata')
load('mouseover_rawclean.Rdata')
mousedata <- rawclean
load('mouseover_rawclean100.Rdata')
mosuedata100 <- rawclean100
load('old_rawclean.Rdata')
olddata <- rawclean
load('old_rawclean100.Rdata')
olddata100 <- rawclean100
View(rawclean100)
View(rawclean100)
